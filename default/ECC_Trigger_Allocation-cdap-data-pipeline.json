{
    "name": "ECC_Trigger_Allocation",
    "description": "Data Pipeline Application",
    "artifact": {
        "name": "cdap-data-pipeline",
        "version": "6.9.1",
        "scope": "SYSTEM"
    },
    "config": {
        "resources": {
            "memoryMB": 2048,
            "virtualCores": 1
        },
        "driverResources": {
            "memoryMB": 2048,
            "virtualCores": 1
        },
        "connections": [],
        "comments": [],
        "postActions": [],
        "properties": {},
        "processTimingEnabled": true,
        "stageLoggingEnabled": false,
        "stages": [
            {
                "name": "Verify inputs",
                "plugin": {
                    "name": "PySparkProgram",
                    "type": "sparkprogram",
                    "label": "Verify inputs",
                    "artifact": {
                        "name": "dynamic-spark",
                        "version": "2.2.3",
                        "scope": "USER"
                    },
                    "properties": {
                        "pythonCode": "from pyspark import *\nfrom pyspark.sql import *\nfrom cdap.pyspark import SparkExecutionContext\nimport google.auth\nfrom google.cloud import storage\nimport traceback\nimport re\nimport logging\n\nsec = SparkExecutionContext()\nsc = SparkContext()\n\n# reading the required properties from runtime arguments\nproject_id = \"${project-id}\"\nextracted_data_bucket_name = \"${extracted-data-bucket}\" if \"gs://\" not in \"${extracted-data-bucket}\" else \"${extracted-data-bucket}\".split(\"gs://\")[1]\nsource = \"${source}\"\ntimestamp = \"${timestamp}\"\nextracted_data_dir = source + \".\" + timestamp\ntarget_files = \"${extracted-files}\"\n\nstorage_client = storage.Client()\nextracted_data_bucket = storage_client.get_bucket(extracted_data_bucket_name)\n  \ndef find_blob(blob_path):\n  blob = extracted_data_bucket.get_blob(blob_path)\n  return blob\n\ndef verify_input(file_path):\n  print(f\"--------------------Verify input file: {file_path}--------------------\")\n  blob = find_blob(file_path)\n  blob_exists = blob.exists()\n  if blob_exists:\n    blob_size = f\"{blob.size} bytes\"\n    if blob_size == \"0 bytes\":\n      print(f\"--------------------Empty File Detected!!!: {blob.name}--------------------\")\n      raise Exception(f\"Verify inputs - Failed for File: {file_path} as it is Empty!\")\n  \ndef verify_all_inputs():\n  targeted_input_files_list = target_files.split(',')\n  print(f\"--------------------List of Targeted Files for 'Verify inputs': {target_files}--------------------\")\n  for target_input_file in targeted_input_files_list:\n    file_path = extracted_data_dir + \"/\" + target_input_file\n    verify_input(file_path)\n    \n\nprint(\"<==================Pre processing execution STARTED================================>\")\nverify_all_inputs()\nprint(\"<==================Pre processing execution COMPLETED==============================>\")"
                    }
                },
                "outputSchema": "",
                "id": "Verify-inputs"
            }
        ],
        "schedule": "0 1 */1 * *",
        "engine": "spark",
        "numOfRecordsPreview": 100,
        "rangeRecordsPreview": {
            "min": 1,
            "max": "5000"
        },
        "description": "Data Pipeline Application",
        "maxConcurrentRuns": 1
    },
    "version": "c9aba657-063f-11ee-b1e7-b6dde77a2af1"
}